{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conformer_with_Libri_Speech_DS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NS8isrn-jcMJ",
        "aEiGQPufjpsm",
        "Vh1SNYiiPRZE",
        "5ZZrfYREWnxJ",
        "lRPyKJauePhQ",
        "p1umEF7cWvqz",
        "LgBF8ZhwWjVA",
        "Dore8cqgeSZg",
        "zIuWPLenuRPb",
        "uUyNRqhVv-_g",
        "1s5jHB_su5CP"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1RnGbp50ju6GboNMQNwv8UazYANXU0-rb",
      "authorship_tag": "ABX9TyPleMOiZxQJzY9vsrEPxV6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcgeo9/Conformer/blob/main/Conformer_with_Libri_Speech_DS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOAD LIBRISPEECH DATASET TO GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "NS8isrn-jcMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { form-width: \"30%\" }\n",
        "%cd drive/MyDrive/Datasets\n",
        "\n",
        "!wget https://openslr.elda.org/resources/12/train-clean-100.tar.gz\n",
        "\n",
        "!tar -xzvf \"/content/drive/MyDrive/Datasets/train-clean-100.tar.gz\" -C \"/content/drive/MyDrive/Datasets/\"     #[run this cell to extract tar.gz files]"
      ],
      "metadata": {
        "id": "neAAjzUajuCw",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE MANIFEST FILE FOR DATASET"
      ],
      "metadata": {
        "id": "aEiGQPufjpsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_df_from_dataset_directory(rootdir,saveloc):\n",
        "  array_of_audio_path=[]\n",
        "  array_of_sentences_files=[]\n",
        "  array_of_sentences=[]\n",
        "  array_of_locations=[]\n",
        "\n",
        "  for subdir, dirs, files in os.walk(rootdir):\n",
        "    files.sort()\n",
        "    for file in files:\n",
        "      filepath=os.path.join(subdir, file)\n",
        "      if (filepath.endswith('.flac')):\n",
        "        array_of_audio_path.append(filepath)\n",
        "      else:\n",
        "        array_of_sentences_files.append(filepath)\n",
        "\n",
        "  for sent_file in array_of_sentences_files:\n",
        "    f = open(sent_file, \"r\")\n",
        "    for line in f:\n",
        "      parts=line.split(' ',1)\n",
        "      if \"\\n\" in parts[1]:\n",
        "        stripped_sent=parts[1].strip()\n",
        "      else:\n",
        "        stripped_sent=parts[1]\n",
        "      stripped_sent=stripped_sent.lower()\n",
        "      array_of_sentences.append(stripped_sent)\n",
        "      array_of_locations.append(parts[0])\n",
        "\n",
        "  #create new df \n",
        "  df = pd.DataFrame({'audio_path':array_of_audio_path,'sentence_location':array_of_locations,'translation':array_of_sentences})\n",
        "\n",
        "  df.to_csv(saveloc+'/manifest_file.csv')\n",
        "  # pd.set_option('max_colwidth', 800)\n",
        "  # pd.set_option('max_columns', 4)\n",
        "  # pd.describe_option('max_colwidth')\n",
        "  # pd.describe_option('max_columns')\n",
        "  # print(df)\n",
        "\n",
        "\n",
        "\n",
        "get_df_from_dataset_directory('/content/drive/MyDrive/Datasets/LibriSpeech/train-clean-100','/content/drive/MyDrive/Datasets/LibriSpeech')"
      ],
      "metadata": {
        "id": "7jlW1y3QEnxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VOCABULARY AND DATALOADERS"
      ],
      "metadata": {
        "id": "GHgMRYFej1Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install ffmpeg\n",
        "\n",
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import spacy  \n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.nn.utils.rnn import pad_sequence \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "spacy_en = spacy.load(\"en\")\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "    \n",
        "    def vocab_to_list(self):\n",
        "        new_list=[*self.stoi]\n",
        "        return new_list\n",
        "\n",
        "    def convert_pred_to_words(self, stoi_pred):\n",
        "      preds=[]\n",
        "      conv= [self.itos[token] if token in self.itos else self.itos[3]\n",
        "          for token in stoi_pred]\n",
        "      # for i in stoi_pred:\n",
        "      #   conv=[]\n",
        "      #   for token in i:\n",
        "      #     if (int(token) in self.itos):\n",
        "      #       add_word=self.itos[int(token)]\n",
        "      #     else:\n",
        "      #       add_word=self.itos[3]\n",
        "      #     conv.append(add_word)\n",
        "\n",
        "        # preds.append(conv)\n",
        "      return conv\n",
        "\n",
        "\n",
        "class EnglishDataset(Dataset):\n",
        "    def __init__(self, root_dir, translation_file, freq_threshold=3):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(translation_file, header=0)\n",
        "\n",
        "        # Get audio path,translation columns\n",
        "        self.audio_path = self.df[\"audio_path\"]\n",
        "        self.translation = self.df[\"translation\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.translation.tolist())\n",
        "        print(\"Vocab Size:\", self.vocab.__len__())\n",
        "\n",
        "        #for specgram\n",
        "        # self.tensor_size=1963\n",
        "        #for mel_specgram\n",
        "        self.tensor_size=392400\n",
        "        self.tensor_trans_size=78\n",
        "        self.pad_idx = self.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getvocab__(self):\n",
        "        return self.vocab\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        translation = self.translation[index]\n",
        "        audio_id = self.audio_path[index]\n",
        "\n",
        "        path_to_audio=self.audio_path[index]\n",
        "\n",
        "        #load audio and normalize between -1 and 1\n",
        "        audio, sample_rate=torchaudio.load(path_to_audio)\n",
        "        audio=torch.squeeze(audio)\n",
        "\n",
        "        #pad audio tensor based on max length calculated manually\n",
        "        values_to_pad=self.tensor_size-len(audio)\n",
        "        m = torch.nn.ConstantPad1d((0, values_to_pad), 0)\n",
        "        padded_audio=m(audio)\n",
        "        \n",
        "        #convert audio to Spectrogram or MelSpectrogram\n",
        "        transform=torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
        "        # specgram = torchaudio.transforms.Spectrogram()(audio)\n",
        "        specgram_WITHNOPAD=transform(audio)\n",
        "        specgram = transform(padded_audio)\n",
        "\n",
        "        #sentences translated to numerical values\n",
        "        numericalized_translation = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_translation += self.vocab.numericalize(translation)\n",
        "        numericalized_translation.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        #list of numerical translation to tensor\n",
        "        translation_tensor=torch.tensor(numericalized_translation)\n",
        "        # print(translation_tensor)\n",
        "\n",
        "        #pad translation tensor based on max length calculated manually\n",
        "        padded_tens_size_trans=self.tensor_trans_size-translation_tensor.shape[0]\n",
        "        p1d=torch.nn.ConstantPad1d( (0, padded_tens_size_trans), self.pad_idx )\n",
        "        padded_tensor_trans=p1d(translation_tensor)\n",
        "\n",
        "        return specgram,specgram_WITHNOPAD.shape[1], padded_tensor_trans, translation_tensor.shape[0]\n",
        "\n",
        "\n",
        "# class MyCollate:\n",
        "#     def __call__(self, batch):\n",
        "#         audio = [item[0].unsqueeze(0) for item in batch]\n",
        "#         audio = torch.cat(audio, dim=0)\n",
        "\n",
        "#         # audio=torch.squeeze(audio)\n",
        "\n",
        "#         targets = [item[1].unsqueeze(0) for item in batch]\n",
        "#         targets = torch.cat(targets, dim=0)\n",
        "\n",
        "#         return audio, targets\n",
        "\n",
        "#number of samples=28539, we use batch size=32 \n",
        "#colab's max workers are 4, but if increased we can use 8\n",
        "def get_loader(root_folder,annotation_file,batch_size=16,num_workers=4,shuffle=True,pin_memory=True,):\n",
        "    dataset = EnglishDataset(root_folder, annotation_file)\n",
        "\n",
        "    words_vocab=dataset.__getvocab__()\n",
        "\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    validation_size = int(0.2 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - validation_size\n",
        "\n",
        "    train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "\n",
        "    ##########################################################################\n",
        "    #FOR GPU\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,num_workers=num_workers,shuffle=True,pin_memory=pin_memory,drop_last=True,)\n",
        "    validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size,num_workers=num_workers,shuffle=False,pin_memory=pin_memory,drop_last=True,)\n",
        "    test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,num_workers=num_workers,shuffle=False,pin_memory=pin_memory,drop_last=True,)\n",
        "\n",
        "    ##########################################################################\n",
        "    #FOR TPU\n",
        "\n",
        "    # import torch_xla\n",
        "    # import torch_xla.core.xla_model as xm\n",
        "\n",
        "\n",
        "    # train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    #       train_dataset,\n",
        "    #       num_replicas=xm.xrt_world_size(),\n",
        "    #       rank=xm.get_ordinal(),\n",
        "    #       shuffle=True)\n",
        "    # validation_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    #       validation_dataset,\n",
        "    #       num_replicas=xm.xrt_world_size(),\n",
        "    #       rank=xm.get_ordinal(),\n",
        "    #       shuffle=False)    \n",
        "    # test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    #       test_dataset,\n",
        "    #       num_replicas=xm.xrt_world_size(),\n",
        "    #       rank=xm.get_ordinal(),\n",
        "    #       shuffle=False)    \n",
        "\n",
        "    # train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,num_workers=num_workers,sampler=train_sampler,pin_memory=pin_memory,drop_last=True,)\n",
        "    # validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size,num_workers=num_workers,sampler=validation_sampler,pin_memory=pin_memory,drop_last=True,)\n",
        "    # test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,num_workers=num_workers,sampler=test_sampler,pin_memory=pin_memory,drop_last=True,)\n",
        "\n",
        "    ##########################################################################\n",
        "\n",
        "\n",
        "    return train_loader, validation_loader, test_loader, dataset, words_vocab\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_loader, validation_loader, test_loader, dataset, words_vocab = get_loader(\"/content/drive/MyDrive/Datasets/LibriSpeech\", \"/content/drive/MyDrive/Datasets/LibriSpeech/manifest_file.csv\")\n",
        "\n",
        "    print(\"Training Loader number of Batches:\", len(train_loader))\n",
        "    print(\"Validation Loader number of Batches:\", len(validation_loader))\n",
        "    print(\"Testing Loader number of Batches:\", len(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IOQq-gYj0-U",
        "outputId": "215f4807-58c7-4908-9260-0f034fb43500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 16121\n",
            "Training Loader number of Batches: 1248\n",
            "Validation Loader number of Batches: 356\n",
            "Testing Loader number of Batches: 178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIND MAX AUDIO LENGTH AND MAX TRANSLATION LENGTH FOR PADDING IN LOADERS"
      ],
      "metadata": {
        "id": "Vh1SNYiiPRZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation_file=\"drive/MyDrive/Datasets/LibriSpeech/manifest_file.csv\"\n",
        "\n",
        "df_test = pd.read_csv(translation_file, header=0)\n",
        "\n",
        "# Get audio path,translation columns\n",
        "audio_path = df_test[\"audio_path\"]\n",
        "translation =df_test[\"translation\"]\n",
        "\n",
        "max_shape=0\n",
        "for i,aud in enumerate(audio_path):\n",
        "  audio, sample_rate=torchaudio.load(aud)\n",
        "  # specgram = torchaudio.transforms.Spectrogram()(audio)\n",
        "  specgram = torchaudio.transforms.MelSpectrogram()(audio)\n",
        "\n",
        "  #if statement to keep track\n",
        "  # if (i % 1000)==0:\n",
        "  #   print(i, specgram.shape)\n",
        "\n",
        "  if (specgram.shape[2]>max_shape):\n",
        "    max_shape=specgram.shape[2]\n",
        "\n",
        "list_of_lens=[]\n",
        "for i,sent in enumerate(translation):\n",
        "  a=[tok.text for tok in spacy_en.tokenizer(sent)]\n",
        "  list_of_lens.append(len(a))\n",
        "  \n",
        "\n",
        "print(max_shape)\n",
        "#+2 because we add SOS and EOS\n",
        "print(max(list_of_lens)+2)"
      ],
      "metadata": {
        "id": "KcNxdlAq7e93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation_file=\"drive/MyDrive/Datasets/LibriSpeech/manifest_file.csv\"\n",
        "\n",
        "df_test = pd.read_csv(translation_file, header=0)\n",
        "\n",
        "# Get audio path,translation columns\n",
        "audio_path = df_test[\"audio_path\"]\n",
        "translation =df_test[\"translation\"]\n",
        "\n",
        "max_shape=392400\n",
        "for i,aud in enumerate(audio_path):\n",
        "\n",
        "  audio, sample_rate=torchaudio.load(aud)\n",
        "  audio=torch.squeeze(audio)\n",
        "\n",
        "  values_to_pad=max_shape-len(audio)\n",
        "  m = torch.nn.ConstantPad1d((0, values_to_pad), 0)\n",
        "\n",
        "  padded_audio=m(audio)\n",
        "  print(padded_audio)\n",
        "  print(padded_audio.shape)\n",
        "\n",
        "  # if (audio.shape[1]>max_shape):\n",
        "  #   max_shape=audio.shape[1]\n",
        "\n",
        "  transform=torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
        "  # transform_pad_one=torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,pad=10)\n",
        "  # specgram = torchaudio.transforms.Spectrogram()(audio)\n",
        "  specgram1 = transform(padded_audio)\n",
        "  # specgram2= transform_pad_one(audio)\n",
        "\n",
        "  print(specgram1.shape)\n",
        "  # print(specgram2.shape)\n",
        "  \n",
        "  break\n",
        "#max shape is calculated=392400\n",
        "print(max_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiZK19BeSveT",
        "outputId": "f841db71-6b18-4cdb-9dc7-44cc75e06607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.1035e-05, 6.1035e-05, 9.1553e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00])\n",
            "torch.Size([392400])\n",
            "torch.Size([128, 1963])\n",
            "392400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PACKAGE INSTALLATION & MODEL TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "lpq1AZ2sEaYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSTALL PACKAGES TO USE TPUS AND TORCHAUDIO (COMBATIBLE)"
      ],
      "metadata": {
        "id": "73ltTRQ3aLBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SETUP CONFORMER FROM https://github.com/sooftware/conformer"
      ],
      "metadata": {
        "id": "5ZZrfYREWnxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE TO INSTALL CONFORMER PACKAGE AUTOMATICALLY\n",
        "# https://github.com/sooftware/conformer\n",
        "\n",
        "!pip install git+https://github.com/sooftware/conformer.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s95qpcnTEdo_",
        "outputId": "48c0a4bc-35f5-40cf-f56a-e552daea0c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/sooftware/conformer.git\n",
            "  Cloning https://github.com/sooftware/conformer.git to /tmp/pip-req-build-xokjnzbr\n",
            "  Running command git clone -q https://github.com/sooftware/conformer.git /tmp/pip-req-build-xokjnzbr\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from conformer===latest) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from conformer===latest) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->conformer===latest) (3.10.0.2)\n",
            "Building wheels for collected packages: conformer\n",
            "  Building wheel for conformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for conformer: filename=conformer-latest-py3-none-any.whl size=18344 sha256=43c103d35ab990a9b051e971b5763e2c78f4c8dd10dcb5d1dbbf3b7a4c8b00db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hug_980a/wheels/58/e3/8f/c80015975bb214b50aca0fcf6449d6f55154176de96c0a3046\n",
            "\u001b[33m  WARNING: Built wheel for conformer is invalid: Metadata 1.2 mandates PEP 440 version, but 'latest' is not\u001b[0m\n",
            "Failed to build conformer\n",
            "Installing collected packages: conformer\n",
            "    Running setup.py install for conformer ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: conformer was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed conformer-latest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE TO INSTALL MANUALLY AND BE ABLE TO MODIFY PACKAGE\n",
        "# https://github.com/sooftware/conformer\n",
        "\n",
        "!git clone https://github.com/sooftware/conformer.git\n",
        "\n",
        "%cd conformer\n",
        "\n",
        "!python3 setup.py install\n",
        "\n",
        "import os, sys\n",
        "sys.path.append(os.getcwd())"
      ],
      "metadata": {
        "id": "L6-NnfSpW2C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SETUP CTC DECODER FROM https://github.com/parlance/ctcdecode"
      ],
      "metadata": {
        "id": "lRPyKJauePhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/parlance/ctcdecode.git"
      ],
      "metadata": {
        "id": "YXlPkAq-eKrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76db537-8201-444f-a3e2-855564e6865f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/parlance/ctcdecode.git\n",
            "  Cloning https://github.com/parlance/ctcdecode.git to /tmp/pip-req-build-ikwdtef4\n",
            "  Running command git clone -q https://github.com/parlance/ctcdecode.git /tmp/pip-req-build-ikwdtef4\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13308100 sha256=c1b91a16b403aef16b1f6e5bd796dee286bfe09d27a84c5c60253d0043a8150d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_f5vx1kd/wheels/ee/a2/65/642d6cea0147b4683da85d09137940b92987cbe132b883c9ca\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SETUP TORCHMETRICS"
      ],
      "metadata": {
        "id": "p1umEF7cWvqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE TO INSTALL METRICS\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "J7taBL-yWxtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c48234-98d2-495b-cd96-e26d2cbd7f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 81 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 102 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 133 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 143 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 153 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 163 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 174 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 184 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 194 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 204 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 215 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 235 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 245 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 256 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 266 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 276 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 286 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 296 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 307 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 317 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 327 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 337 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 348 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 358 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 368 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 378 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 389 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 398 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Collecting pyDeprecate==0.3.*\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.7)\n",
            "Installing collected packages: pyDeprecate, torchmetrics\n",
            "Successfully installed pyDeprecate-0.3.2 torchmetrics-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SETUP TPU WITH TORCHAUDIO"
      ],
      "metadata": {
        "id": "LgBF8ZhwWjVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "!pip install torch==1.9.0+cu111 torchaudio -f https://download.pytorch.org/whl/cu111/torch_stable.html"
      ],
      "metadata": {
        "id": "flCiZFdESyCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OTHER"
      ],
      "metadata": {
        "id": "Dore8cqgeSZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html -U"
      ],
      "metadata": {
        "id": "E1Jd2Bdxbhoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.models import Conformer\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "cuda = torch.cuda.is_available()  \n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "conformer = Conformer(input_dim=128, num_heads=4, ffn_dim=144, num_layers=8, depthwise_conv_kernel_size=31,dropout=0.1).to(device)\n",
        "\n",
        "for i, (audio,audio_len, translations, translation_len) in enumerate(train_loader):\n",
        "  input=torch.transpose(audio, 1, 2).to(device)\n",
        "  lengths=audio_len.to(device)\n",
        "  print(lengths.shape)\n",
        "  lengths=torch.full((4,1), 1963).squeeze().to(device)\n",
        "  print(input.shape)\n",
        "  print(lengths.shape)\n",
        "  \n",
        "  output,out_len = conformer(input, lengths)\n",
        "  print(output)\n",
        "  break\n",
        "\n",
        "\n",
        "\n",
        "# lengths = torch.randint(1, 400, (10,))  # (batch,)\n",
        "# input = torch.rand(10, int(lengths.max()), 128)  # (batch, num_frames, input_dim)\n",
        "# print(input)\n",
        "# print(lengths)\n",
        "# output,out_len = conformer(input, lengths)\n",
        "# print(output.shape)"
      ],
      "metadata": {
        "id": "WIwMzND436Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "jKzRBCBoBijX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LEARNING RATE SCHEDULER"
      ],
      "metadata": {
        "id": "zIuWPLenuRPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''A wrapper class for scheduled optimizer '''\n",
        "import numpy as np\n",
        "\n",
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.lr_mul = lr_mul\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.lr_mul * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "wOeS3z_suTwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScheduleOptimizer():\n",
        "  \"Optim wrapper that implements rate.\"\n",
        "  def __init__(self, model_size, warmup, optimizer):\n",
        "    self.optimizer = optimizer\n",
        "    self._step = 0\n",
        "    self.warmup = warmup\n",
        "    self.model_size = model_size\n",
        "    self._rate = 0\n",
        "  \n",
        "  def state_dict(self):\n",
        "    \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n",
        "    It contains an entry for every variable in self.__dict__ which\n",
        "    is not the optimizer.\n",
        "    \"\"\"\n",
        "    return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "  \n",
        "  def load_state_dict(self, state_dict):\n",
        "    \"\"\"Loads the warmup scheduler's state.\n",
        "    Arguments:\n",
        "        state_dict (dict): warmup scheduler state. Should be an object returned\n",
        "            from a call to :meth:`state_dict`.\n",
        "    \"\"\"\n",
        "    self.__dict__.update(state_dict) \n",
        "\n",
        "  def zero_grad(self):\n",
        "    \"Zero out the gradients with the inner optimizer\"\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "  def step(self):\n",
        "    \"Update parameters and rate\"\n",
        "    self._step += 1\n",
        "    rate = self.rate()\n",
        "    for p in self.optimizer.param_groups:\n",
        "        p['lr'] = rate\n",
        "    self._rate = rate\n",
        "    self.optimizer.step()\n",
        "      \n",
        "  def rate(self, step = None):\n",
        "    \"Implement `lrate` above\"\n",
        "    if step is None:\n",
        "        step = self._step\n",
        "    return (self.model_size ** (-0.5) *min(step ** (-0.5), step * self.warmup ** (-1.5))) "
      ],
      "metadata": {
        "id": "9aDVNdfV6H6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BEAM SEARCH DECODER"
      ],
      "metadata": {
        "id": "uUyNRqhVv-_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "decoder = CTCBeamDecoder(\n",
        "    words_vocab.vocab_to_list(),\n",
        "    model_path=None,\n",
        "    alpha=0,\n",
        "    beta=0,\n",
        "    cutoff_top_n=40,\n",
        "    cutoff_prob=1.0,\n",
        "    beam_width=100,\n",
        "    num_processes=4,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True\n",
        ")"
      ],
      "metadata": {
        "id": "qsxEdHVWsPb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def greedy_search_decoder(predictions):\n",
        "  \n",
        "    #select token with the maximum probability for each prediction\n",
        "    output_sequence = [np.argmax(prediction) for prediction in predictions]\n",
        "    \n",
        "    #storing token probabilities\n",
        "    token_probabilities = [np.max(prediction) for prediction in predictions]\n",
        "    \n",
        "    #multiply individaul token-level probabilities to get overall sequence probability\n",
        "    sequence_probability = np.product(token_probabilities)\n",
        "    \n",
        "    return output_sequence, sequence_probability\n",
        "    \n",
        "# model_prediction = [[0.1, 0.7, 0.1, 0.1],\n",
        "#                     [0.7, 0.1, 0.1, 0.1],\n",
        "#                     [0.1, 0.1, 0.6, 0.2],\n",
        "#                     [0.1, 0.1, 0.1, 0.7],\n",
        "#                     [0.4, 0.3, 0.2, 0.1]]\n",
        "\n",
        "# greedy_search_decoder(model_prediction)\n",
        "\n",
        "def beam_search_decoder(predictions, top_k = 3):\n",
        "    #start with an empty sequence with zero score\n",
        "    output_sequences = [([], 0)]\n",
        "    \n",
        "    #looping through all the predictions\n",
        "    for token_probs in predictions:\n",
        "        new_sequences = []\n",
        "        \n",
        "        #append new tokens to old sequences and re-score\n",
        "        for old_seq, old_score in output_sequences:\n",
        "            for char_index in range(len(token_probs)):\n",
        "                new_seq = old_seq + [char_index]\n",
        "                #considering log-likelihood for scoring\n",
        "                new_score = old_score + token_probs[char_index]\n",
        "                new_sequences.append((new_seq, new_score))\n",
        "                \n",
        "        #sort all new sequences in the de-creasing order of their score\n",
        "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
        "        \n",
        "        #select top-k based on score \n",
        "        # *Note- best sequence is with the highest score\n",
        "        output_sequences = output_sequences[:top_k]\n",
        "        \n",
        "    return output_sequences\n",
        "\n",
        "# model_prediction = [[0.1, 0.7, 0.1, 0.1],\n",
        "#                     [0.7, 0.1, 0.1, 0.1],\n",
        "#                     [0.1, 0.1, 0.6, 0.2],\n",
        "#                     [0.1, 0.1, 0.1, 0.7],\n",
        "#                     [0.4, 0.3, 0.2, 0.1]]\n",
        "                    \n",
        "# beam_search_decoder(model_prediction, top_k = 5)\n",
        "\n",
        "\n",
        "# beam search\n",
        "def beam_search_decoder_test(data, k):\n",
        "\tsequences = [[list(), 0.0]]\n",
        "\t# walk over each step in sequence\n",
        "\tfor row in data:\n",
        "\t\tall_candidates = list()\n",
        "\t\t# expand each current candidate\n",
        "\t\tfor i in range(len(sequences)):\n",
        "\t\t\tseq, score = sequences[i]\n",
        "\t\t\tfor j in range(len(row)):\n",
        "\t\t\t\tcandidate = [seq + [j], score - row[j]]\n",
        "\t\t\t\tall_candidates.append(candidate)\n",
        "\t\t# order all candidates by score\n",
        "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\t\t# select k best\n",
        "\t\tsequences = ordered[:k]\n",
        "\treturn sequences"
      ],
      "metadata": {
        "id": "8DY-xGqeLrfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decoder_torch(post, k):\n",
        "    \"\"\"Beam Search Decoder\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "        post(Tensor) – the posterior of network.\n",
        "        k(int) – beam size of decoder.\n",
        "\n",
        "    Outputs:\n",
        "\n",
        "        indices(Tensor) – a beam of index sequence.\n",
        "        log_prob(Tensor) – a beam of log likelihood of sequence.\n",
        "\n",
        "    Shape:\n",
        "\n",
        "        post: (batch_size, seq_length, vocab_size).\n",
        "        indices: (batch_size, beam_size, seq_length).\n",
        "        log_prob: (batch_size, beam_size).\n",
        "\n",
        "    Examples:\n",
        "\n",
        "        >>> post = torch.softmax(torch.randn([32, 20, 1000]), -1)\n",
        "        >>> indices, log_prob = beam_search_decoder(post, 3)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, seq_length, _ = post.shape\n",
        "    log_post = post\n",
        "    log_prob, indices = log_post[:, 0, :].topk(k, sorted=True)\n",
        "    indices = indices.unsqueeze(-1)\n",
        "    for i in range(1, seq_length):\n",
        "        log_prob = log_prob.unsqueeze(-1) + log_post[:, i, :].unsqueeze(1).repeat(1, k, 1)\n",
        "        log_prob, index = log_prob.view(batch_size, -1).topk(k, sorted=True)\n",
        "        indices = torch.cat([indices, index.unsqueeze(-1)], dim=-1)\n",
        "    return indices, log_prob"
      ],
      "metadata": {
        "id": "mgD8ZZEC4OqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TPU TRAIN"
      ],
      "metadata": {
        "id": "1s5jHB_su5CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import sys\n",
        "from google.colab import output\n",
        "import torch.nn as nn\n",
        "from conformer import Conformer\n",
        "import torchmetrics\n",
        "import random\n",
        "# import torchaudio.models\n",
        "\n",
        "def _run(flags):\n",
        "  cuda = torch.cuda.is_available()  \n",
        "  # device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "\n",
        "  def train_model(model, optimizer, criterion, loader, device, metric):\n",
        "    running_loss = 0.0\n",
        "    for i, (audio,audio_len, translations, translation_len) in enumerate(loader):\n",
        "      output.clear(output_tags='some_outputs')\n",
        "      with output.use_tags('some_outputs'):\n",
        "        sys.stdout.write('Batch: '+ str(i+1)+'/'+str(len(loader)))\n",
        "        sys.stdout.flush();\n",
        "\n",
        "      #sorting inputs and targets to have targets in descending order based on len\n",
        "      sorted_list,sorted_indices=torch.sort(translation_len,descending=True)\n",
        "\n",
        "      sorted_audio=torch.zeros(audio.shape,dtype=torch.float)\n",
        "      sorted_audio_len=torch.zeros(audio_len.shape,dtype=torch.int)\n",
        "      sorted_translations=torch.zeros(translations.shape,dtype=torch.int)\n",
        "      sorted_translation_len=sorted_list\n",
        "\n",
        "      for index, contentof in enumerate(translation_len):\n",
        "        sorted_audio[index]=audio[sorted_indices[index]]\n",
        "        sorted_audio_len[index]=audio_len[sorted_indices[index]]\n",
        "        sorted_translations[index]=translations[sorted_indices[index]]\n",
        "\n",
        "      #transpose inputs from (batch, dim, seq_len) to (batch, seq_len, dim)\n",
        "      inputs=sorted_audio.to(device)\n",
        "      inputs=torch.transpose(inputs, 1, 2)\n",
        "      input_lengths=sorted_audio_len\n",
        "      targets=sorted_translations.to(device)\n",
        "      target_lengths=sorted_translation_len\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward propagate\n",
        "      outputs, output_lengths = model(inputs, input_lengths)\n",
        "\n",
        "      # Calculate CTC Loss\n",
        "      loss = criterion(outputs.transpose(0, 1), targets, output_lengths, target_lengths)\n",
        "\n",
        "      loss.backward()\n",
        "      # optimizer.step()\n",
        "\n",
        "      xm.optimizer_step(optimizer)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    loss_per_epoch=running_loss/(i+1)\n",
        "    # print(f'Loss: {loss_per_epoch:.3f}')\n",
        "\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    return loss_per_epoch\n",
        "\n",
        "  ################################################################################\n",
        "\n",
        "  def eval_model(model, optimizer, criterion, loader, device, metric):\n",
        "    running_loss = 0.0\n",
        "    wer_calc=0.0\n",
        "    random_index_per_epoch= random.randint(0, 178)\n",
        "\n",
        "    for i, (audio,audio_len, translations, translation_len) in enumerate(loader):\n",
        "      output.clear(output_tags='some_outputs')\n",
        "      with output.use_tags('some_outputs'):\n",
        "        sys.stdout.write('Batch: '+ str(i+1)+'/'+str(len(loader)))\n",
        "        sys.stdout.flush();\n",
        "\n",
        "      #sorting inputs and targets to have targets in descending order based on len\n",
        "      sorted_list,sorted_indices=torch.sort(translation_len,descending=True)\n",
        "\n",
        "      sorted_audio=torch.zeros(audio.shape,dtype=torch.float)\n",
        "      sorted_audio_len=torch.zeros(audio_len.shape,dtype=torch.int)\n",
        "      sorted_translations=torch.zeros(translations.shape,dtype=torch.int)\n",
        "      sorted_translation_len=sorted_list\n",
        "\n",
        "      for index, contentof in enumerate(translation_len):\n",
        "        sorted_audio[index]=audio[sorted_indices[index]]\n",
        "        sorted_audio_len[index]=audio_len[sorted_indices[index]]\n",
        "        sorted_translations[index]=translations[sorted_indices[index]]\n",
        "\n",
        "      #transpose inputs from (batch, dim, seq_len) to (batch, seq_len, dim)\n",
        "      inputs=sorted_audio.to(device)\n",
        "      inputs=torch.transpose(inputs, 1, 2)\n",
        "      input_lengths=sorted_audio_len\n",
        "      targets=sorted_translations.to(device)\n",
        "      target_lengths=sorted_translation_len\n",
        "\n",
        "      # Forward propagate\n",
        "      outputs, output_lengths = model(inputs, input_lengths)\n",
        "\n",
        "      # predictions=greedy_search_decoder(outputs)\n",
        "\n",
        "      # Calculate CTC Loss\n",
        "      loss = criterion(outputs.transpose(0, 1), targets, output_lengths, target_lengths)\n",
        "\n",
        "      # for i in outputs:\n",
        "        # model_pred=greedy_search_decoder(i.cpu().detach().numpy())\n",
        "      ind,prob=beam_search_decoder_torch(outputs.cpu().detach(),5)\n",
        "\n",
        "      # print(ind.shape)\n",
        "      print(ind)\n",
        "      # print(prob.shape)\n",
        "      print(prob)\n",
        "          # print(len(model_pred[0]))\n",
        "\n",
        "      # print(outputs.transpose(0,1)[0])\n",
        "      # outputs_in_words=words_vocab.convert_pred_to_words(outputs.transpose(0, 1)[0])\n",
        "      # targets_in_words=words_vocab.convert_pred_to_words(targets)\n",
        "      # wer=metrics_calculation(metric, outputs_in_words,targets_in_words)\n",
        "      \n",
        "\n",
        "      # if (i==random_index_per_epoch):\n",
        "      #     print(outputs_in_words,targets_in_words)\n",
        "\n",
        "      # if (i % 100)==0:\n",
        "      #   xm.master_print(f'Batch: {i}/{len(loader)}')\n",
        "\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      # wer_calc += wer\n",
        "\n",
        "    loss_per_epoch=running_loss/(i+1)\n",
        "    wer_per_epoch=wer_calc/(i+1)\n",
        "\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    return loss_per_epoch, wer_per_epoch\n",
        "\n",
        "  ################################################################################\n",
        "\n",
        "  def train_eval_model(epochs):\n",
        "\n",
        "    device=xm.xla_device()\n",
        "    print('Device:', device)\n",
        "\n",
        "    inputdim=dataset[0][0].shape[0]\n",
        "    print('Input Dimensions:', inputdim)\n",
        "\n",
        "    #conformer model init\n",
        "    model = nn.DataParallel(Conformer(num_classes=16121, input_dim=inputdim, encoder_dim=144, num_encoder_layers=16, num_attention_heads=4, conv_kernel_size=31)).to(device)\n",
        "\n",
        "    # Optimizers specified in the torch.optim package\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    #loss function\n",
        "    criterion = nn.CTCLoss().to(device)\n",
        "\n",
        "    #metrics init\n",
        "    metric=torchmetrics.WordErrorRate()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      start = time.time()\n",
        "\n",
        "      print(\"Epoch\", epoch+1)\n",
        "\n",
        "      ############################################################################\n",
        "      #TRAINING   \n",
        "\n",
        "      model.train()\n",
        "      print(\"Training\")\n",
        "\n",
        "      epoch_loss=train_model(model=model,optimizer=optimizer, criterion=criterion, loader=train_loader, device=device, metric=metric)\n",
        "\n",
        "      print(format(epoch_loss, \".4f\")) \n",
        "      # print(f'WER: {epoch_wer:.3f}')\n",
        "\n",
        "      ############################################################################\n",
        "      #EVALUATION\n",
        "\n",
        "      with torch.no_grad():\n",
        "        model.train(False)\n",
        "\n",
        "        print(\"Validation\")\n",
        "\n",
        "        epoch_val_loss, epoch_val_wer=eval_model(model=model,optimizer=optimizer, criterion=criterion, loader=validation_loader, device=device, metric=metric)\n",
        "        \n",
        "        print(format(epoch_val_loss, \".4f\")) \n",
        "        # print(f'WER: {epoch_val_wer:.3f}')   \n",
        "\n",
        "      print(f'Epoch {epoch+1} completed in {(time.time() - start)/60} minutes')\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  def metrics_calculation(metric, predictions, targets):\n",
        "      print(predictions)\n",
        "      print(targets)\n",
        "      wer=metric(predictions, targets)\n",
        "\n",
        "      return wer\n",
        "  \n",
        "  train_eval_model(1)\n",
        "\n",
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    a = _run(flags)\n",
        "\n",
        "flags={}\n",
        "xmp.spawn(_mp_fn, args=(flags,), nprocs=1, start_method='fork')\n",
        "\n",
        "\n",
        "\n",
        "# train_eval_model(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT1iJ3UxElaa",
        "outputId": "a2cdf478-993a-4650-8825-78284f743833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: xla:1\n",
            "Input Dimensions: 128\n",
            "Epoch 1\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 10/4994"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU TRAIN"
      ],
      "metadata": {
        "id": "IpJbGBhtu8rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import sys\n",
        "from google.colab import output\n",
        "import torch.nn as nn\n",
        "from conformer import Conformer\n",
        "import torchmetrics\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def train_model(model, optimizer, criterion, loader, device, metric):\n",
        "  running_loss = 0.0\n",
        "  running_wer=0.0\n",
        "  for i, (audio,audio_len, translations, translation_len) in enumerate(loader):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    with output.use_tags('some_outputs'):\n",
        "      sys.stdout.write('Batch: '+ str(i+1)+'/'+str(len(loader)))\n",
        "      sys.stdout.flush();\n",
        "\n",
        "    #sorting inputs and targets to have targets in descending order based on len\n",
        "    sorted_list,sorted_indices=torch.sort(translation_len,descending=True)\n",
        "\n",
        "    sorted_audio=torch.zeros(audio.shape,dtype=torch.float)\n",
        "    sorted_audio_len=torch.zeros(audio_len.shape,dtype=torch.int)\n",
        "    sorted_translations=torch.zeros(translations.shape,dtype=torch.int)\n",
        "    sorted_translation_len=sorted_list\n",
        "\n",
        "    for index, contentof in enumerate(translation_len):\n",
        "      sorted_audio[index]=audio[sorted_indices[index]]\n",
        "      sorted_audio_len[index]=audio_len[sorted_indices[index]]\n",
        "      sorted_translations[index]=translations[sorted_indices[index]]\n",
        "\n",
        "    #transpose inputs from (batch, dim, seq_len) to (batch, seq_len, dim)\n",
        "    inputs=sorted_audio.to(device)\n",
        "    inputs=torch.transpose(inputs, 1, 2)\n",
        "    input_lengths=sorted_audio_len\n",
        "    targets=sorted_translations.to(device)\n",
        "    target_lengths=sorted_translation_len\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward propagate\n",
        "    outputs, output_lengths = model(inputs, input_lengths)\n",
        "\n",
        "    # Calculate CTC Loss\n",
        "    loss = criterion(outputs.transpose(0, 1), targets, output_lengths, target_lengths)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Beam Search Decoding\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(outputs,output_lengths)\n",
        "    # beam_results, beam_scores, timesteps, out_lens = decoder.decode(logP, logits_len)\n",
        "\n",
        "    # Batch Pred List\n",
        "    batch_pred_list = []\n",
        "    word_err_rate_batch=0\n",
        "\n",
        "    # Batch loop\n",
        "    for b in range(outputs.size(0)):\n",
        "      result_beam=beam_results[b][0][:out_lens[b][0]].tolist()\n",
        "      batch_pred_list.append(result_beam)\n",
        "      \n",
        "      word_error_rate=metrics_calculation(metric,result_beam,targets[b])\n",
        "      word_err_rate_batch+=word_error_rate\n",
        "\n",
        "    # print(batch_pred_list)\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    running_wer+=word_err_rate_batch/outputs.size(0)\n",
        "\n",
        "  loss_per_epoch=running_loss/(i+1)\n",
        "  wer_per_epoch=running_wer/(i+1)\n",
        "\n",
        "  output.clear(output_tags='some_outputs')\n",
        "  return loss_per_epoch, wer_per_epoch\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def eval_model(model, optimizer, criterion, loader, device, metric):\n",
        "  running_loss = 0.0\n",
        "  running_wer=0.0\n",
        "\n",
        "  for i, (audio,audio_len, translations, translation_len) in enumerate(loader):\n",
        "    output.clear(output_tags='some_outputs')\n",
        "    with output.use_tags('some_outputs'):\n",
        "      sys.stdout.write('Batch: '+ str(i+1)+'/'+str(len(loader)))\n",
        "      sys.stdout.flush();\n",
        "\n",
        "    #sorting inputs and targets to have targets in descending order based on len\n",
        "    sorted_list,sorted_indices=torch.sort(translation_len,descending=True)\n",
        "\n",
        "    sorted_audio=torch.zeros(audio.shape,dtype=torch.float)\n",
        "    sorted_audio_len=torch.zeros(audio_len.shape,dtype=torch.int)\n",
        "    sorted_translations=torch.zeros(translations.shape,dtype=torch.int)\n",
        "    sorted_translation_len=sorted_list\n",
        "\n",
        "    for index, contentof in enumerate(translation_len):\n",
        "      sorted_audio[index]=audio[sorted_indices[index]]\n",
        "      sorted_audio_len[index]=audio_len[sorted_indices[index]]\n",
        "      sorted_translations[index]=translations[sorted_indices[index]]\n",
        "\n",
        "    #transpose inputs from (batch, dim, seq_len) to (batch, seq_len, dim)\n",
        "    inputs=sorted_audio.to(device)\n",
        "    inputs=torch.transpose(inputs, 1, 2)\n",
        "    input_lengths=sorted_audio_len\n",
        "    targets=sorted_translations.to(device)\n",
        "    target_lengths=sorted_translation_len\n",
        "\n",
        "    # Forward propagate\n",
        "    outputs, output_lengths = model(inputs, input_lengths)\n",
        "\n",
        "    # predictions=greedy_search_decoder(outputs)\n",
        "\n",
        "    # Calculate CTC Loss\n",
        "    loss = criterion(outputs.transpose(0, 1), targets, output_lengths, target_lengths)\n",
        "\n",
        "    # ind,prob=beam_search_decoder_torch(outputs.cpu().detach(),5)\n",
        "    # print(ind.shape)\n",
        "    # print(ind)\n",
        "    # print(prob.shape)\n",
        "    # print(prob)\n",
        "\n",
        "    # Beam Search Decoding\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(outputs,output_lengths)\n",
        "\n",
        "    # Batch Pred List\n",
        "    batch_pred_list = []\n",
        "    word_err_rate_batch=0\n",
        "\n",
        "    # Batch loop\n",
        "    for b in range(outputs.size(0)):\n",
        "      result_beam=beam_results[b][0][:out_lens[b][0]].tolist()\n",
        "      batch_pred_list.append(result_beam)\n",
        "      \n",
        "      word_error_rate=metrics_calculation(metric,result_beam,targets[b])\n",
        "      word_err_rate_batch+=word_error_rate\n",
        "\n",
        "    # print(outputs.transpose(0,1)[0])\n",
        "    # outputs_in_words=words_vocab.convert_pred_to_words(outputs.transpose(0, 1)[0])\n",
        "    # targets_in_words=words_vocab.convert_pred_to_words(targets)\n",
        "    # wer=metrics_calculation(metric, outputs_in_words,targets_in_words)\n",
        "    \n",
        "\n",
        "    # if (i==random_index_per_epoch):\n",
        "    #     print(outputs_in_words,targets_in_words)\n",
        "\n",
        "    # if (i % 100)==0:\n",
        "    #   xm.master_print(f'Batch: {i}/{len(loader)}')\n",
        "\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    running_wer+=word_err_rate_batch/outputs.size(0)\n",
        "\n",
        "  loss_per_epoch=running_loss/(i+1)\n",
        "  wer_per_epoch=running_wer/(i+1)\n",
        "\n",
        "  output.clear(output_tags='some_outputs')\n",
        "  return loss_per_epoch, wer_per_epoch\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def train_eval_model(epochs):\n",
        "\n",
        "  path = \"/content/drive/MyDrive/Datasets/model.pt\"\n",
        "\n",
        "  # tensorboard writer\n",
        "  writer = SummaryWriter(\"/content/drive/MyDrive/Datasets/tensorboard\")\n",
        "\n",
        "  # device setup\n",
        "  cuda = torch.cuda.is_available()  \n",
        "  device = torch.device('cuda' if cuda else 'cpu')\n",
        "  print('Device:', device)\n",
        "\n",
        "  inputdim=dataset[0][0].shape[0]\n",
        "  print('Input Dimensions:', inputdim)\n",
        "\n",
        "  #conformer model init\n",
        "  model = nn.DataParallel(Conformer(num_classes=16121, input_dim=inputdim, encoder_dim=32, num_encoder_layers=2, num_attention_heads=4, conv_kernel_size=31)).to(device)\n",
        "\n",
        "  # Optimizers specified in the torch.optim package\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "  # optimizer = ScheduleOptimizer(optimizer=torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09), model_size=144, warmup=10000)\n",
        "\n",
        "  #loss function\n",
        "  criterion = nn.CTCLoss().to(device)\n",
        "\n",
        "  #metrics init\n",
        "  metric=torchmetrics.WordErrorRate()\n",
        "\n",
        "  if os.path.exists(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    starting_point = checkpoint['epoch']+1\n",
        "    loss = checkpoint['loss']\n",
        "    print(\"Model Checkpoint Found:\")\n",
        "    print(\"Starting epoch:\", starting_point)\n",
        "    print(\"Current Loss:\", loss)\n",
        "  else:\n",
        "    print(\"No model Checkpoint found\")\n",
        "    starting_point=1\n",
        "\n",
        "  for epoch in range(starting_point,epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    ############################################################################\n",
        "    #TRAINING   \n",
        "\n",
        "    model.train()\n",
        "    print(\"Training\")\n",
        "\n",
        "    epoch_loss,epoch_wer=train_model(model=model,optimizer=optimizer, criterion=criterion, loader=train_loader, device=device, metric=metric)\n",
        "\n",
        "    writer.add_scalar(\"Loss/Train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"WER/Train\", epoch_wer, epoch)\n",
        "    print(\"Loss:\", format(epoch_loss, \".4f\")) \n",
        "    print(\"WER:\", format(epoch_wer, \".4f\"))\n",
        "\n",
        "    ############################################################################\n",
        "    #EVALUATION\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.train(False)\n",
        "\n",
        "      print(\"Validation\")\n",
        "\n",
        "      epoch_val_loss, epoch_val_wer=eval_model(model=model,optimizer=optimizer, criterion=criterion, loader=validation_loader, device=device, metric=metric)\n",
        "      \n",
        "      writer.add_scalar(\"Loss/Validation\", epoch_val_loss, epoch)\n",
        "      writer.add_scalar(\"WER/Validation\", epoch_val_wer, epoch)\n",
        "      print(\"Loss:\", format(epoch_val_loss, \".4f\")) \n",
        "      print(\"WER:\", format(epoch_val_wer, \".4f\"))\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss,\n",
        "            }, path)\n",
        "    \n",
        "    print(f'Epoch {epoch} completed in {(time.time() - start)/60} minutes')\n",
        "  writer.flush()\n",
        "  writer.close()\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def metrics_calculation(metric, predictions, targets):\n",
        "  pred_sent=' '.join(words_vocab.convert_pred_to_words(predictions))\n",
        "  targ_sent=' '.join(words_vocab.convert_pred_to_words(targets.tolist()))\n",
        "\n",
        "  wer=metric(pred_sent, targ_sent)\n",
        "\n",
        "  return wer.item()\n",
        "\n",
        "train_eval_model(100)"
      ],
      "metadata": {
        "id": "yDOZYcj3u_zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9537d4-fd14-4277-a706-e0eaf2f12696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Input Dimensions: 128\n",
            "Model Checkpoint Found:\n",
            "Starting epoch: 50\n",
            "Current Loss: 3.7623177480239134\n",
            "Epoch 50\n",
            "Training\n",
            "Loss: 3.7447\n",
            "WER: 0.8467\n",
            "Validation\n",
            "Loss: 3.4365\n",
            "WER: 0.8211\n",
            "Epoch 50 completed in 94.42254243691762 minutes\n",
            "Epoch 51\n",
            "Training\n",
            "Loss: 3.7133\n",
            "WER: 0.8452\n",
            "Validation\n",
            "Loss: 3.4309\n",
            "WER: 0.8203\n",
            "Epoch 51 completed in 91.84995356003444 minutes\n",
            "Epoch 52\n",
            "Training\n",
            "Loss: 3.6878\n",
            "WER: 0.8443\n",
            "Validation\n",
            "Loss: 3.4354\n",
            "WER: 0.8193\n",
            "Epoch 52 completed in 91.59916432301203 minutes\n",
            "Epoch 53\n",
            "Training\n",
            "Loss: 3.6572\n",
            "WER: 0.8427\n",
            "Validation\n",
            "Loss: 3.3956\n",
            "WER: 0.8164\n",
            "Epoch 53 completed in 91.73973194758098 minutes\n",
            "Epoch 54\n",
            "Training\n",
            "Loss: 3.6356\n",
            "WER: 0.8415\n",
            "Validation\n",
            "Loss: 3.3998\n",
            "WER: 0.8167\n",
            "Epoch 54 completed in 92.15700422525406 minutes\n",
            "Epoch 55\n",
            "Training\n",
            "Loss: 3.6089\n",
            "WER: 0.8401\n",
            "Validation\n",
            "Loss: 3.3708\n",
            "WER: 0.8137\n",
            "Epoch 55 completed in 92.37225886185963 minutes\n",
            "Epoch 56\n",
            "Training\n",
            "Loss: 3.5898\n",
            "WER: 0.8391\n",
            "Validation\n",
            "Loss: 3.3517\n",
            "WER: 0.8137\n",
            "Epoch 56 completed in 93.88647603193918 minutes\n",
            "Epoch 57\n",
            "Training\n",
            "Loss: 3.5670\n",
            "WER: 0.8375\n",
            "Validation\n",
            "Loss: 3.3489\n",
            "WER: 0.8121\n",
            "Epoch 57 completed in 95.51203623215358 minutes\n",
            "Epoch 58\n",
            "Training\n",
            "Loss: 3.5454\n",
            "WER: 0.8367\n",
            "Validation\n",
            "Loss: 3.3159\n",
            "WER: 0.8101\n",
            "Epoch 58 completed in 95.48525347709656 minutes\n",
            "Epoch 59\n",
            "Training\n",
            "Loss: 3.5265\n",
            "WER: 0.8354\n",
            "Validation\n",
            "Loss: 3.3145\n",
            "WER: 0.8100\n",
            "Epoch 59 completed in 95.2335187872251 minutes\n",
            "Epoch 60\n",
            "Training\n",
            "Loss: 3.5041\n",
            "WER: 0.8344\n",
            "Validation\n",
            "Loss: 3.2852\n",
            "WER: 0.8076\n",
            "Epoch 60 completed in 95.70492793718974 minutes\n",
            "Epoch 61\n",
            "Training\n",
            "Loss: 3.4876\n",
            "WER: 0.8335\n",
            "Validation\n",
            "Loss: 3.2823\n",
            "WER: 0.8079\n",
            "Epoch 61 completed in 95.69430297613144 minutes\n",
            "Epoch 62\n",
            "Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 1241/1248"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TESTING"
      ],
      "metadata": {
        "id": "nqmFKRwuugRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import sys\n",
        "from google.colab import output\n",
        "import torch.nn as nn\n",
        "from conformer import Conformer\n",
        "# import torchmetrics\n",
        "import random\n",
        "\n",
        "path = \"/content/drive/MyDrive/Datasets/model.pt\"\n",
        "\n",
        "# device setup\n",
        "cuda = torch.cuda.is_available()  \n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "inputdim=dataset[0][0].shape[0]\n",
        "print('Input Dimensions:', inputdim)\n",
        "\n",
        "#conformer model init\n",
        "model = nn.DataParallel(Conformer(num_classes=16121, input_dim=inputdim, encoder_dim=144, num_encoder_layers=16, num_attention_heads=4, conv_kernel_size=31)).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "# optimizer = ScheduleOptimizer(optimizer=torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09), model_size=144, warmup=10000)\n",
        "\n",
        "#loss function\n",
        "criterion = nn.CTCLoss().to(device)\n",
        "\n",
        "#metrics init\n",
        "# metric=torchmetrics.WordErrorRate()\n",
        "\n",
        "if os.path.exists(path):\n",
        "  checkpoint = torch.load(path, map_location=device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  starting_point = checkpoint['epoch']+1\n",
        "  loss = checkpoint['loss']\n",
        "  print(\"Model Checkpoint Found\")\n",
        "  # print(\"Starting epoch:\", starting_point)\n",
        "  # print(\"Current Loss:\", loss)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.train(False)\n",
        "\n",
        "  for i, (audio,audio_len, translations, translation_len) in enumerate(test_loader):\n",
        "    #sorting inputs and targets to have targets in descending order based on len\n",
        "    sorted_list,sorted_indices=torch.sort(translation_len,descending=True)\n",
        "\n",
        "    sorted_audio=torch.zeros(audio.shape,dtype=torch.float)\n",
        "    sorted_audio_len=torch.zeros(audio_len.shape,dtype=torch.int)\n",
        "    sorted_translations=torch.zeros(translations.shape,dtype=torch.int)\n",
        "    sorted_translation_len=sorted_list\n",
        "\n",
        "    for index, contentof in enumerate(translation_len):\n",
        "      sorted_audio[index]=audio[sorted_indices[index]]\n",
        "      sorted_audio_len[index]=audio_len[sorted_indices[index]]\n",
        "      sorted_translations[index]=translations[sorted_indices[index]]\n",
        "\n",
        "    #transpose inputs from (batch, dim, seq_len) to (batch, seq_len, dim)\n",
        "    inputs=sorted_audio.to(device)\n",
        "    inputs=torch.transpose(inputs, 1, 2)\n",
        "    input_lengths=sorted_audio_len\n",
        "    targets=sorted_translations.to(device)\n",
        "    target_lengths=sorted_translation_len\n",
        "\n",
        "    # Forward propagate\n",
        "    outputs, output_lengths = model(inputs, input_lengths)\n",
        "\n",
        "    # print(beam_results[4][0][:out_lens[4][0]])\n",
        "\n",
        "    # Beam Search Decoding\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(outputs,output_lengths)\n",
        "    # beam_results, beam_scores, timesteps, out_lens = decoder.decode(logP, logits_len)\n",
        "\n",
        "    # Batch Pred List\n",
        "    batch_pred_list = []\n",
        "\n",
        "    # Batch loop\n",
        "    for b in range(outputs.size(0)):\n",
        "        batch_pred_list.append(beam_results[b][0][:out_lens[b][0]].tolist())\n",
        "\n",
        "    print(batch_pred_list)\n",
        "\n",
        "    # print(beam_scores)\n",
        "\n",
        "    # transl_test=words_vocab.convert_pred_to_words(beam_results[0][0])\n",
        "    # print(transl_test)\n",
        "    # predictions,chance=beam_search_decoder_torch(outputs,3)\n",
        "    # print(predictions[torch.argmax(chance[0])])\n",
        "    # print(chance[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGNkWfXpr33Y",
        "outputId": "25b00899-2b96-4b19-b42b-cb19886e4029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Input Dimensions: 128\n",
            "Model Checkpoint Found\n",
            "[[1, 3, 4, 3, 3, 3, 3, 3, 2], [1, 3, 3, 4, 3, 3, 4, 2, 3, 2], [1, 3, 3, 3, 4, 3, 3, 3, 2], [1, 3, 3, 4, 3, 3, 3, 3, 2], [1, 3, 3, 3, 3, 3, 3, 2], [1, 3, 3, 3, 3, 2, 3, 2], [1, 4, 3, 4, 4, 4, 2], [1, 3, 4, 3, 4, 3, 3, 3, 2], [1, 3, 4, 4], [1, 3, 3, 3, 3, 3, 3, 2], [1, 3, 3, 4, 3, 4, 3, 4, 2, 3, 2], [1, 3, 3, 3, 3], [1, 3, 3, 4, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]]\n"
          ]
        }
      ]
    }
  ]
}